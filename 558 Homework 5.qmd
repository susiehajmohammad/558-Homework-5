---
title: "558 Homework 5"
author: "Susan Hajmohammad"
format: pdf
---

## Task 1

-   Question 1: What is the purpose of using cross-validation when fitting a random forest model? The purpose of using cross validation when fitting a random forest model is to rotate through the data partitions so each one has a turn testing the model. That way we can see how well the random forest model performs on new data multiple times.

-   Question 2: Describe the bagged tree algorithm. Bagged tree algorithm is bootstrapping samples then aggregating. We would make some new datasets using the bootstrapping method (with replacement, non-parametric), then create a full tree on each new dataset. We then average the results of the trees and in theory the averaged results are more reliable than just making one tree.

-   Question 3: What is meant by a general linear model? A general linear model is a regression model where the model is generally Y= intercept + betas\*x's + an error term. You can have SLR, MLR and ANOVA models too.

-   Question 4: When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model? An interaction term looks at how two variables affect the response together. When the model doesn't include an interaction term, the model is just looking at how the variables affect the response independently.

-   Question 5: Why do we split our data into a training and test set? That way we have a chunk of data that we didn't train the model on, so we can see how it does with predicting new data it hasn't seen yet. If we just used all the data to train the model we wouldn't have data to test it with!

## Task 2

### Packages and Data

```{r, message=FALSE}
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)

heart_data <- read_csv("heart.csv")

```

### Question 1

```{r}
summary(heart_data)
```

-   a) What type of variable (in R) is Heart Disease? Categorical or Quantitative?
  Heart disease appears to be quantitative. 

-   b)Does this make sense? Why or why not.
  This doesn't really make sense since Heart Disease is supposed to be a binary response like True or False.  

### Question 2

```{r}
new_heart <- heart_data %>%
  mutate(heart_disease = as.factor(HeartDisease))%>%
  select(-HeartDisease, -ST_Slope)

summary(new_heart)
```

## Task 3

### Question 1
```{r}
#colorblind friendly scatterplot for age as function of heart disease
#palette from cookbook-r.com 
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

p <- ggplot(data = new_heart, mapping = aes(x = MaxHR, y = Age, color = heart_disease))

p+ geom_point() + geom_smooth(method = "lm") +  scale_colour_manual(values=cbPalette)
```

### Question 2
Based on the graph visually, I think there is evidence for interaction because the two lines aren't parrallel and cross each other. 

## Task 4

### Split data into training and test set: 

```{r}
set.seed(101)
new_heart_split <- initial_split(new_heart, prop = 0.8)

test <- testing(new_heart_split)
  
train <- training(new_heart_split)

```

## Task 5

### Question 1
```{r}
# fit interaction model named ols_mlr

ols_mlr <- lm(Age ~ MaxHR*heart_disease, data = train)
summary(ols_mlr)

```

### Question 2

```{r}
test_model <- predict(ols_mlr, newdata = test)

# calculation for RMSE
sqrt(mean((test$Age - test_model)^2))


```


### Question 3

```{r}

#LASSO recipe

LASSO_recipe <- recipe(Age ~ MaxHR + heart_disease, data = train) %>%
  step_dummy(heart_disease) %>%
  step_normalize(all_numeric_predictors())%>%
  step_interact(~MaxHR:starts_with("heart_disease_"))

LASSO_recipe

```


### Question 4

```{r}
#model spec
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet") |>
  set_mode("regression")

#tuning grid
lambda_grid <- grid_regular(penalty(), levels = 30)

#lasso workflow
lasso_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(lasso_spec)

#Cv folds
set.seed(101)
cv_splits <- vfold_cv(train, v = 10)

#tune model on grid

lasso_fit <- lasso_wkf |>
  tune_grid(
    resamples = cv_splits,
    grid      = lambda_grid,
    metrics   = metric_set(rmse))

#selecting best penalty
lowest_rmse <- lasso_fit |> 
  select_best(metric =  "rmse")

#fit lasso on all training data
final_lasso <- lasso_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(data = train)

#final coefficients
tidy(final_lasso)


```



### Question 5
Without even looking, I'd expect them to be roughly the same because the penalty is almost 0 (above). So the LASSO barely shrank the coefficients from their original values in the OLS, i think the test data RMSE will be almost the same for both. 
### Question 6
```{r}
ols_rmse <- rmse_vec(
  truth    = test$Age,
  estimate = predict(ols_mlr, newdata = test)
)
ols_rmse

lasso_rmse <- rmse_vec(
  truth    = test$Age,
  estimate = predict(final_lasso, new_data = test)$.pred
)
lasso_rmse


```

### Question 7
Because the cross validation penalty is almost 0.  That means the shrinkage is doing almost nothing to the lasso coefficients.  


## Task 6

### Question 1

### Question 2

### Question 3
